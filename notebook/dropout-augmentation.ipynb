{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38d397d1-860d-4ca9-8b0b-9fe2113347f1",
    "_kg_hide-output": false,
    "_uuid": "20e67fe0e36a283ab6e899c9d45693e608dd5bdd"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# import third-party library\n",
    "sys.path.append('./my_lib/')\n",
    "from data_augmentation import DataAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "csv_train = pd.read_csv('../input/labels.csv')\n",
    "csv_test = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fe883370-0270-4ad4-a8c0-05984af2900a",
    "_uuid": "6af41eb2be0b3d3f92a10a9f8bdf5fd7ae8bf94b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read training CSV\n",
    "csv_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "639d3a2e-ef8c-4f27-b1a2-243cd5bc9ca9",
    "_uuid": "fc183bdf820d5f5daf0cd263868e1e3e774cb3a8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read test csv\n",
    "csv_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cc0fce21-3b53-42bf-8d0f-291eb020644f",
    "_uuid": "0c6118243a97075dd268239e31f7c851a9aac56b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate Labels\n",
    "targets_series = pd.Series(csv_train['breed'])\n",
    "# print(targets_series)\n",
    "one_hot = pd.get_dummies(targets_series, sparse = True)\n",
    "labels = np.asarray(one_hot)\n",
    "n_check = random.randint(0, len(labels)-1)\n",
    "print(csv_train['breed'][n_check], 'is encoded as', ''.join((str(i) for i in labels[n_check])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d2da7283-968a-42ec-95e0-2ecc42d9b027",
    "_uuid": "c5c96565ee76a1e9f22fcb9fd6f8305c73f05ad4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_size = 90\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "07f84e2f-600a-47ee-9e7e-48ebac184cd5",
    "_uuid": "4478950c6500c326586489713b2c74c2ccf798e2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (f, breed) in enumerate(tqdm(csv_train.values)):\n",
    "    img = cv2.imread('../input/train/{}.jpg'.format(f))\n",
    "    x_train.append(cv2.resize(img, (im_size, im_size)))\n",
    "    y_train.append(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use external module to execute data augmentation.\n",
    "The module execute:\n",
    "- [ ] Inversion\n",
    "- [ ] Sobel derivative\n",
    "- [ ] Scharr derivative\n",
    "- [ ] Laplacian <!--**(error not used for now)**-->\n",
    "- [ ] Blur\n",
    "- [ ] Gaussian blur [disable]\n",
    "- [ ] Median blur\n",
    "- [ ] Bilateral blur\n",
    "- [x] Horizontal flips\n",
    "- [x] Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, images in enumerate(tqdm(DataAugmentation(x_train,\n",
    "                                                 options={'inverse': False,\n",
    "                                                          'sobel_derivative': False,\n",
    "                                                          'scharr_derivative': False,\n",
    "                                                          'laplacian': False,\n",
    "                                                          'blur': False,\n",
    "                                                          'gaussian_blur': False,\n",
    "                                                          'median_blur': False,\n",
    "                                                          'bilateral_blur': False,\n",
    "                                                          'horizontal_flips': True,\n",
    "                                                          'rotation': True,\n",
    "                                                          # 'rotation_config': [(10,1.2)],\n",
    "                                                          'shuffle_result': False}))):\n",
    "    for image in images:\n",
    "        if i == 4:\n",
    "            plt.imshow(image, cmap = 'gray', interpolation = 'bicubic')\n",
    "            plt.show()\n",
    "        x_train.append(image)\n",
    "        y_train.append(y_train[i])\n",
    "    \n",
    "print('dataset became:', len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7fd00196-2c2a-4264-b36c-10351dfc6781",
    "_uuid": "002beaed2ff53c46df2e47f3051ab19225c13f91",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check train\n",
    "n_check = random.randint(0, len(y_train)-1)\n",
    "print('label:', ''.join((str(i) for i in y_train[n_check])))\n",
    "plt.imshow(x_train[n_check], cmap = 'gray', interpolation = 'bicubic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bad58384-b970-4820-b5d1-168c31d6b863",
    "_uuid": "036e033fbb2f962bdcf03723d85fd0658a3635ce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in tqdm(csv_test['id'].values):\n",
    "    img = cv2.imread('../input/test/{}.jpg'.format(f))\n",
    "    x_test.append(cv2.resize(img, (im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build np array and normalise them\n",
    "x_train_raw = np.array(x_train, np.float32) / 255.\n",
    "y_train_raw = np.array(y_train, np.uint8)\n",
    "x_test_raw  = np.array(x_test, np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape:\", x_train_raw.shape)\n",
    "print(\"y_train shape:\", y_train_raw.shape)\n",
    "print(\"x_test shape:\", x_test_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y_train_raw.shape[1]\n",
    "classes = csv_test.columns.values[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the __stratify__ parameter on __treain_test_split__ the split should be equally distributed per classes.\n",
    "\n",
    "**TODO:** Add cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x_train_raw, y_train_raw,\n",
    "                                                      test_size=0.20, random_state=42,\n",
    "                                                      stratify=y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base pre-trained model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(im_size, im_size, 3))\n",
    "\n",
    "# Add a new top layer\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# First: train only the top layers (which were randomly initialized)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(30, kernel_initializer='normal', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=40, batch_size=48, \n",
    "                    validation_data=(X_valid, Y_valid), \n",
    "                    callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test_raw, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# check predict\n",
    "n_check = random.randint(0, len(x_test_raw)-1)\n",
    "plt.imshow(x_test_raw[n_check], cmap = 'gray_r', interpolation = 'bicubic')\n",
    "plt.show()\n",
    "pre = model.predict(np.array([x_test_raw[n_check]]))\n",
    "arg_max = np.argmax(pre)\n",
    "print(np.max(pre), arg_max, labels[arg_max])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
